# -*- coding: utf-8 -*-
"""InformeFinal_DeepLearning2Gru_LabIII.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Whbqi2-SrC9mXW403G-mhD5Z881T6c8_

Materia: Laboratorio 3

Grupo: Deep Learning 2

Integrantes:    
- Diaz, Dario
- Figueroa, Martin
- Mazzola, Gabriel

# Hipótesis Experimental

La implementación de un modelo de Unidades Recurrentes con Puerta (GRU) para la predicción de ventas mensuales de productos (alimenticios, cuidado personal y limpieza del hogar), utilizando datos históricos de ventas y características de los productos, mejorará la precisión de las predicciones en comparación con métodos tradicionales de pronóstico.

Las principales acciones realizadas para testear la hipótesis:

***1) Análisis exploratorio de datos (EDA):***

Se cargaron y analizaron múltiples conjuntos de datos relacionados con ventas y productos. Se examinaron las distribuciones de variables y se identificaron posibles anomalías. Se analizaron las correlaciones entre variables.


***2) Preparación de datos:***

Se agruparon los datos por producto y período. Se normalizaron las características para cada producto. Se dividieron los productos en 6 grupos según la disponibilidad de datos.


***3) Implementación del modelo GRU:***

Se desarrolló un modelo GRU para cada grupo de productos. Se utilizaron técnicas de optimización de hiperparámetros. Se implementó un enfoque de predicción directa para el segundo mes.


***4) Experimentación y validación:***

Se utilizaron múltiples semillas para garantizar la reproducibilidad. Se realizaron predicciones para cada grupo de productos. Se promediaron las predicciones de diferentes ejecuciones.


***5) Post-procesamiento y análisis:***

Se ajustaron las predicciones para mantener proporciones y sumas totales consistentes. Se realizaron ajustes adicionales basados en interpretaciones del contexto económico.


***6) Evaluación y comparación:***

Se subieron múltiples predicciones a Kaggle para evaluar el rendimiento.
Se compararon los resultados con el puntaje de referencia en el tablero público.

# Resumen Ejecutivo

En el marco de la competencia Kaggle de predicción de ventas, nuestro equipo realizó un total de 235 submissions, logrando una mejora significativa en el rendimiento. Partiendo de un score inicial de 0.989, alcanzamos un mejor resultado de 0.234 en el Public Leaderboard.
Este informe presenta nuestro enfoque más efectivo, resultado de la consolidación de diversos modelos desarrollados individualmente por los miembros del equipo. Utilizamos un modelo de Unidades Recurrentes con Puerta (GRU), adaptado a diferentes grupos de productos según la disponibilidad de datos históricos.
Nuestro proceso incluyó un extenso análisis exploratorio de datos, preparación de datos específica por producto, optimización de hiperparámetros, y técnicas de post-procesamiento. El código adjunto detalla paso a paso nuestra metodología, reflejando nuestro mejor esfuerzo dado nuestro nivel de experiencia en ciencia de datos y aprendizaje automático

# Analisis Exploratorio de los datos (EDA)

## Carga de datos

En esta primera parte del codigo consolidamos los diferentes archivos y DataFrames creados para la ejecucion del Ejercicio, como asi tambien compartimos algunas de las técnicas de visualización y analisis de estrucutura, anomalías y relaciones entre las variables que hemos experimentado.
"""

# Cargamos nuestro drive

from google.colab import drive
drive.mount('/content/drive')

# Definir el directorio de los scripts y datos

import pandas as pd
import os
import numpy as np

directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'

#Cargamos los archivos con los cuales estaremos trabajando

sell_in = pd.read_csv(os.path.join(directorio_script, 'sell-in.txt.gz'), delimiter='\t')
tb_productos = pd.read_csv(os.path.join(directorio_script, 'tb_productos.txt'), delimiter='\t')
tb_stocks = pd.read_csv(os.path.join(directorio_script, 'tb_stocks.txt.gz'), delimiter='\t')
productos_predecir = pd.read_csv(os.path.join(directorio_script, 'productos_a_predecir.txt'), delimiter='\t')
tb_productos_descripcion = pd.read_csv(os.path.join(directorio_script, 'tb_productos_descripcion.txt'), delimiter='\t')

#Visualizamos uno de ellos para verificar la carga fue correcta
sell_in

# Agrupamos el DataFrame sell_in por las columnas periodo y product_id, sumando los valores en la columna tn y restableciendo el índice para convertir los datos agrupados de nuevo en un DataFrame regular con un índice entero predeterminado.
sell_in= sell_in.groupby(['periodo', 'product_id'])['tn'].sum().reset_index()
sell_in["tn"].sum()

#Visualizamos el df resultante
sell_in

# Verificamos la carga del archivo tb_productos
tb_productos

# Verificamos la carga del archivo tb_productos_descripcion
tb_productos_descripcion

tb_stocks

tb_stocks_order = tb_stocks.sort_values(by=['stock_final'])
tb_stocks_order

# Combinar sell_in_df con tb_productos_df en la columna 'product_id'
# Utilizamos un merge tipo 'left' para conservar todos los registros en sell_in_df incluso si no encuentran correspondencia en tb_productos_df
sell_productos_df = pd.merge(sell_in, tb_productos_descripcion, on='product_id', how="left")
sell_productos_df = sell_productos_df.groupby(['periodo', 'product_id', 'tn']).first().reset_index()

# Visualisamos el nuevo data frame
sell_productos_df

#Calculamos la suma total de todos los valores en la columna "tn" del DataFrame
sell_productos_df["tn"].sum()

# Ahora combinamos el DataFrame resultante con tb_stocks_df en 'product_id' y 'periodo'
# Usamos 'left' de nuevo para mantener todos los registros del DataFrame combinado, aunque no coincidan en tb_stocks_df
final_df_dario = pd.merge(sell_productos_df, tb_stocks, on=['product_id', 'periodo'], how='left')

#Verificamos como quedo el df
final_df_dario

#Calculamos la suma total de todos los valores en la columna "tn" del DataFrame
final_df_dario["tn"].sum()

# Convertir la columna 'product_id' a una lista
productos_especificados = productos_predecir['product_id'].tolist()

# Filtramos el dataset sell_in para los productos relevantes
df_productos_especificados= final_df_dario[final_df_dario['product_id'].isin(productos_especificados)]
df_productos_especificados

# Convertimos la columna periodo del DataFrame a un objeto datetime, utilizando el formato específico '%Y%m'.
df_productos_especificados['periodo'] = pd.to_datetime(df_productos_especificados['periodo'].astype(str), format='%Y%m')

#Ajustamos las fechas en la columna periodo del DataFrame df_productos_especificados al último día del mes correspondiente
df_productos_especificados['periodo'] = df_productos_especificados['periodo'] + pd.offsets.MonthEnd(0)

df_productos_especificados

ventas_df_productos_especificados= df_productos_especificados.groupby(['periodo', 'product_id'])['tn'].sum()
ventas_df_productos_especificados= ventas_df_productos_especificados.reset_index()
ventas_df_productos_especificados

"""## **Distribución de Variables**

1- Queremos conocer tanto la forma del DataFrame como los tipos de datos de cada columna.
"""

# Verificamos la forma del dataset
print(f"Dataset contains {df_productos_especificados.shape[0]} rows and {df_productos_especificados.shape[1]} columns.")

# Verificamos los tipos de datos de cada columna
print(df_productos_especificados.dtypes)

"""Generamos un resumen estadístico para las columnas numéricas y categóricas"""

# Columnas numéricas
print(df_productos_especificados.describe())

# Columnas Categóricas
print(df_productos_especificados.describe(include=['object']))

"""*   **Distribución de tn**: **La columna tn tiene una gran variabilidad**, con valores que van desde un mínimo de 0.00089 hasta un máximo de 2295.20. La media es 50.23 y la desviación estándar es 125.02, lo que sugiere que hay algunos valores extremadamente altos que están afectando la media

*   **Concentración en Categorías y Marcas**: La mayoría de los registros se concentran en la categoría PC y subcategoría CABELLO, con la marca NIVEA siendo la más frecuente.

*   Tamaño de SKU: El tamaño promedio de SKU (sku_size) es 447.64, con una desviación estándar alta (831.24). Esto indica una gran variabilidad en los tamaños de los productos

*   Posibles Errores en Datos de stock_final:
El valor mínimo de stock_final es -13.67, lo cual es un dato inusual y probablemente erróneo, ya que no tiene sentido tener un inventario final negativo

Visualizamos los datos con el fin de identificar y comprender distribuciones y relaciones.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Seleccionamos sólo columnas numéricas para correlación
numerical_df = df_productos_especificados.select_dtypes(include=['number'])

# Histograma para columnas numericas
numerical_df.hist(bins=30, figsize=(15, 10))
plt.show()

"""*  La distribución de tn es altamente sesgada hacia valores más bajos, con la mayoría de los valores concentrados cerca de cero

*   El gráfico de sku_size muestra una distribución muy sesgada con la mayoría de los tamaños de SKU concentrados en valores bajos y pocos en valores alto

*  El stock_final también muestra una distribución sesgada, con la mayoría de los registros reportando bajos niveles de stock final y algunos valores significativamente alto

## Correlación de variables
"""

# Mapa de Calor/Correlation heatmap para columnas numericas
plt.figure(figsize=(12, 8))
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')
plt.show()

"""*   **Relación entre tn y stock_final**: La correlación positiva moderada (0.52) sugiere que un mayor volumen (tn) está asociado con un mayor stock_final. Esto podría indicar que productos con mayores ventas tienden a mantener más inventario final o que los productos más populares requieren más stock para satisfacer la demanda.

*   **Relación Inversa entre product_id y tn**:La correlación negativa moderada (-0.46) entre product_id y tn podría indicar que los productos introducidos más recientemente (con IDs más altos) tienden a tener menores volúmenes. Esto podría ser porque los productos nuevos aún no han alcanzado su pleno potencial de ventas.

*   La correlación negativa débil con sku_size y stock_final puede sugerir que los productos más nuevos (con IDs más altos) tienden a tener tamaños de SKU más pequeños y menor stock final.

## Análisis de Valores Atípicos
"""

plt.figure(figsize=(10, 6))
sns.boxplot(x=df_productos_especificados['tn'])
plt.title('Boxplot de Toneladas Vendidas (tn)')
plt.xlabel('Toneladas')
plt.show()

"""Algunas observaciones de este gráfico:

1.   La mayor parte de los datos se concentra cerca del valor mínimo, lo que sugiere que l**a mayoría de los productos tienen ventas bajas en toneladas**
2.   **Hay muchos valores atípicos que se extienden hasta más de 2000 toneladas**. Estos valores atípicos pueden influir en las estadísticas resumen y en los modelos predictivos.
3.  La presencia de tantos valores atípicos sugiere que hay productos con ventas muy altas en toneladas que son significativamente diferentes del resto.



"""

plt.figure(figsize=(10, 6))
sns.boxplot(x=df_productos_especificados['stock_final'])
plt.title('Boxplot de Stock Final')
plt.xlabel('Stock Final')
plt.show()

"""1.   Similar al boxplot anterior, podemos
observar la presencia de muchos valores atípicos en el Stock Final.
2.   Existen muchos valores atípicos que se extienden hasta más de 1500 unidades de stock.

## **Análisis de Valores Faltantes**

Nos interesa encontrar huecos en la continuidad de los datos que puedan afectar el codigo a utilizar
"""

# Verificamos missing values
missing_values = df_productos_especificados.isnull().sum()
print("Missing values in each column:\n", missing_values)

"""En resumen, **todas las columnas excepto stock_final están completas** y no tienen valores faltantes. La columna stock_final, sin embargo, tiene una cantidad significativa de valores faltantes, específicamente 11,622 de los 22,349 registros totales del DataFrame. Esto representa aproximadamente el 52% de los valores en esa columna."""

# Contar valores faltantes por columna
print(df_productos_especificados.isnull().sum())

# Visualizar los valores faltantes con un heatmap (opcional, requiere seaborn)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(df_productos_especificados.isnull(), cbar=False, cmap='viridis')
plt.show()

#En el DataFrame de las ventas de los productos especificados buscamos si existen productos con meses faltantes de datos
import pandas as pd

ventas_df_productos_especificados = pd.DataFrame(ventas_df_productos_especificados)

# Convert periodo to datetime
ventas_df_productos_especificados['periodo'] = pd.to_datetime(ventas_df_productos_especificados['periodo'])

# Generate a complete date range for the given period
date_range = pd.date_range(start='2017-01-31', end='2019-12-31', freq='M')

# Create a MultiIndex with all combinations of product_id and date_range
product_ids = ventas_df_productos_especificados['product_id'].unique()
multi_index = pd.MultiIndex.from_product([product_ids, date_range], names=['product_id', 'periodo'])

# Reindex the original dataframe to include all combinations of product_id and date_range
full_df = ventas_df_productos_especificados.set_index(['product_id', 'periodo']).reindex(multi_index).reset_index()

# Identify missing rows
missing_dates = full_df[full_df['tn'].isna()][['product_id', 'periodo']].rename(columns={'periodo': 'missing_periodo'})

# Display the missing dates
print("Missing Dates for Each Product")
print(missing_dates)

# Graficamos "missing dates" por producto

import plotly.express as px

# Create a new DataFrame with product_id and missing_periodo
missing_dates_df = missing_dates[['product_id', 'missing_periodo']]

# Plot the missing dates by product ID
fig = px.scatter(missing_dates_df, x="product_id", y="missing_periodo",
                 hover_data=["missing_periodo"],
                 title="Missing Dates by Product ID",
                 labels={"product_id": "Product ID", "missing_periodo": "Missing Period"})

fig.show()

# Listamos los productos con faltante de datos en "missing_dates_df"

missing_product_ids = missing_dates_df['product_id'].tolist()

print("List of product_id with missing data:")
print(missing_product_ids)

# Contamos los "product_id" en missing_product_ids

missing_product_ids_counts = {}

for product_id in missing_product_ids:
  if product_id not in missing_product_ids_counts:
    missing_product_ids_counts[product_id] = 0
  missing_product_ids_counts[product_id] += 1

# Print the count of missing dates for each product ID
print("Count of Missing Dates for Each Product ID:")
for product_id, count in missing_product_ids_counts.items():
  print(f"Product ID: {product_id}, Count: {count}")

# Hacemos un listado de los product_id con periodos faltantes en 2018 y 2019

# Convert 'missing_periodo' to datetime if it's not already
missing_dates['missing_periodo'] = pd.to_datetime(missing_dates['missing_periodo'])

# Filter missing dates for years 2018 and 2019
missing_dates_2018_2019 = missing_dates[missing_dates['missing_periodo'].dt.year.isin([2018, 2019])]

# Get the unique product_id values
missing_product_ids_2018_2019 = missing_dates_2018_2019['product_id'].unique()

# Print the list of product_id
print("List of product_id with missing periods in 2018 and 2019:")
print(missing_product_ids_2018_2019)

# Agrupamos los productos segun los meses que les faltan en missing_product_ids_2018_2019

import pandas as pd

# Convert 'missing_periodo' to datetime if it's not already
missing_dates['missing_periodo'] = pd.to_datetime(missing_dates['missing_periodo'])

# Filter missing dates for years 2018 and 2019
missing_dates_2018_2019 = missing_dates[missing_dates['missing_periodo'].dt.year.isin([2018, 2019])]

# Get the unique product_id values
missing_product_ids_2018_2019 = missing_dates_2018_2019['product_id'].unique()

# Create an empty dictionary to store the groups
product_groups = {}

# Loop through each product_id
for product_id in missing_product_ids_2018_2019:
  # Get the missing periods for the current product_id
  missing_periods = missing_dates_2018_2019[missing_dates_2018_2019['product_id'] == product_id]['missing_periodo'].tolist()

  # Convert the list of Timestamps to a tuple. Tuples are immutable and hence hashable
  missing_periods_tuple = tuple(missing_periods)

  # Check if the missing periods are already in the groups dictionary
  if missing_periods_tuple not in product_groups:
    # If not, create a new group with the missing periods and add the product_id
    product_groups[missing_periods_tuple] = [product_id]
  else:
    # If the missing periods are already in the dictionary, append the product_id to the existing group
    product_groups[missing_periods_tuple].append(product_id)

# Print the product groups
print("Product Groups based on Missing Months:")
for missing_periods, product_ids in product_groups.items():
  print(f"Missing Periods: {missing_periods}")
  print(f"Product IDs: {product_ids}")
  print()

# Filtramos esas ventas para el año 2019

ventas_2019_df = ventas_df_productos_especificados[ventas_df_productos_especificados['periodo'].dt.year == 2019]
ventas_2019_df

"""En base a este analisis decidimos devidir los datos en 6 grupos segun la disponibilidad y continuidad de los datos. Estos 6 grupos son:
- Productos con 6 meses de datos continuos
- Productos con 12 meses de datos continuos
- prodcutos con 18 meses de datos continuos
- productos com 24 meses de datos continuos
- Prodcutos con datos continuos entre 18 y 24 meses
- Productos con datos discontinuos entre 18 y 24 meses

## **Analisis** **Estadístico**

### Análisis de categorías
"""

plt.figure(figsize=(10, 6))
sns.countplot(df_productos_especificados['cat1'])
plt.title('Distribución de Categorías (cat1)')
plt.xlabel('Categoría 1')
plt.ylabel('Frecuencia')
plt.xticks(rotation=90)
plt.show()

"""La categoría PC domina el dataset con la mayor cantidad de productos. Esto sugiere que el mercado de cuidado personal es una parte significativa del inventario y ventas.

HC y FOODS también son importantes, mientras que REF tiene muy poca frencuencia.
"""

plt.figure(figsize=(10, 6))
sns.countplot(df_productos_especificados['cat2'])
plt.title('Distribución de Categorías (cat2)')
plt.xlabel('Categoría 2')
plt.ylabel('Frecuencia')
plt.xticks(rotation=90)
plt.show()

"""*   CABELLO es la categoría más frecuente, con una presencia significativamente mayor que las otras categorías.
*   HOGAR, SOPAS Y CALDOS, DEOS, PIEL2, y ADERAZOS también tienen una alta frecuencia, mientras que ROPA MANCHAS, OTROS, PROFESIONAL, DENTAL, y TE podrían representar oportunidades de crecimient
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Supongamos que ya tienes el DataFrame 'df_productos_especificados' cargado

# Contar las ocurrencias de cada categoría y ordenar
cat3_counts = df_productos_especificados['cat3'].value_counts().sort_values(ascending=False)

# Crear un DataFrame temporal para asegurar el orden
sorted_cat3 = cat3_counts.index.tolist()
df_sorted_cat3 = df_productos_especificados[df_productos_especificados['cat3'].isin(sorted_cat3)]

# Graficar
plt.figure(figsize=(10, 10))  # Ajustar el tamaño de la figura para mejor visibilidad
sns.countplot(data=df_sorted_cat3, y='cat3', order=sorted_cat3)
plt.title('Distribución de Categorías (cat3)')
plt.xlabel('Frecuencia')
plt.ylabel('Categoría 3')
plt.xticks(rotation=0)  # Rotar las etiquetas de x para que se vean claramente
plt.show()

"""*   Las categorías SHAMPOO, ACONDICIONADOR, Aerosoles, Sopa, Líquidos y MAyonesa"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Supongamos que ya tienes el DataFrame 'df_productos_especificados' cargado

# Contar las ocurrencias de cada marca y ordenar
brand_counts = df_productos_especificados['brand'].value_counts().sort_values(ascending=False)

# Crear un DataFrame temporal para asegurar el orden
sorted_brands = brand_counts.index.tolist()
df_sorted_brands = df_productos_especificados[df_productos_especificados['brand'].isin(sorted_brands)]

# Graficar
plt.figure(figsize=(14, 8))  # Ajustar el tamaño de la figura para mejor visibilidad
sns.countplot(data=df_sorted_brands, y='brand', order=sorted_brands)
plt.title('Distribución de Marcas')
plt.xlabel('Frecuencia')
plt.ylabel('Marca')
plt.xticks(rotation=0)  # Rotar las etiquetas de x para que se vean claramente
plt.show()

"""*   **La distribución no es uniforme**, con algunas marcas dominando claramente el mercado mientras otras tienen una presencia mucho menor.

*   Las marcas con mayor frecuencia son:
MAGGI, NIVEA, SHAMPOO3, MUSCULO, DEOS1

*   Las marcas de cuidado personal y productos de limpieza parecen ser predominantes en esta distribución (por ejemplo, NIVEA, SHAMPOO, COLGATE, DEOS).

### Análisis de product_id
"""

# Graficamos "tn" versus "time" para el TOP 20 de Productos con el mayor "tn" acumulado

# Calculate the total sales ("tn") for each product ID
total_sales_per_product = df_productos_especificados.groupby('product_id')['tn'].sum().sort_values(ascending=False)

# Select the top 20 product IDs with the highest accumulated "tn"
top_20_product_ids = total_sales_per_product.index[:20].tolist()

# Filter the dataset to only include the top 20 product IDs
top_20_df = df_productos_especificados[df_productos_especificados['product_id'].isin(top_20_product_ids)]

# Create a figure with subplots
fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(15, 10))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Initialize a counter to track the current subplot
ax_count = 0

# Loop through the top 20 product IDs
for product_id in top_20_product_ids:
  # Select data for the current product ID
  product_data = top_20_df[top_20_df['product_id'] == product_id]

  # Plot tn vs time
  axes[ax_count].plot(product_data['periodo'], product_data['tn'])
  axes[ax_count].set_title(f'Product ID: {product_id}')
  axes[ax_count].set_xlabel('Time')
  axes[ax_count].set_ylabel('Total Sales')

  # Increment the counter
  ax_count += 1

  # Check if we've reached the last subplot
  if ax_count == len(axes):
    break

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

"""Se **observan productos con distintos comportamientos:**

*  **Crecimiento** constante:20001 y 20002

*  **Estacionalidad o ciclos regulares**:20003, 20004, 20005

*  Volatilidad alta:20006, 20013, 20014

*  Declive gradual: 20007,20008,20012.


*  Estabilidad relativa: 20009,20015, 20016

*  Pico único seguido de estabilización:20010, 20019

*  Fluctuaciones moderadas sin tendencia clara:20011, 20017, 20018, 20020

### Análisis de stock_final

Los niveles de stock varían significativamente entre productos. Por ejemplo, el producto 20004 alcanza cerca de 1500 unidades, mientras que otros como el 20012 no superan las 150 unidades.
"""

# Graficamos los 12 productos con más "tn" vs "stock_final"

# Create a figure with subplots
fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(15, 10))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Initialize a counter to track the current subplot
ax_count = 0

# Loop through unique product IDs
for product_id in df_productos_especificados['product_id'].unique():
  # Select data for the current product ID
  product_data = df_productos_especificados[df_productos_especificados['product_id'] == product_id]

  # Plot stock_final vs time
  axes[ax_count].plot(product_data['periodo'], product_data['stock_final'])
  axes[ax_count].set_title(f'Product ID: {product_id}')
  axes[ax_count].set_xlabel('Time')
  axes[ax_count].set_ylabel('Stock Final')

  # Increment the counter
  ax_count += 1

  # Check if we've reached the last subplot
  if ax_count == len(axes):
    break

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

"""
*   **Tendencias Estacionales**: Algunos productos, como el ID 20001, 20005, y 20010, muestran picos que podrían corresponder a tendencias estacionales o promociones específicas que impulsan temporalmente el inventario.

*   Tendencias de **Crecimiento o Decrecimiento**: El Producto ID 20004 muestra una tendencia creciente en el nivel de stock, lo que podría ser un indicador de acumulación de inventario posiblemente debido a sobreproducción o disminución en la demanda.

*   Muchos presentan **fluctuaciones sin una tendencia clara**"""

# Graficamos de manera combinada "tn" and "stock final" versus tiempo para el TOP 20 de productos con el mayor "tn" acumulado

# Create a figure with subplots
fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 10))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Initialize a counter to track the current subplot
ax_count = 0

# Find the top 20 product IDs with highest accumulated "tn"
top_20_product_ids = (
    df_productos_especificados.groupby('product_id')['tn'].sum()
    .sort_values(ascending=False)
    .head(20)
    .index.tolist()
)

# Loop through the top 20 product IDs
for product_id in top_20_product_ids:
  # Select data for the current product ID
  product_data = df_productos_especificados[df_productos_especificados['product_id'] == product_id]

  # Plot tn vs time
  axes[ax_count].plot(product_data['periodo'], product_data['tn'], label='Total Sales')
  axes[ax_count].set_title(f'Product ID: {product_id}')
  axes[ax_count].set_xlabel('Time')
  axes[ax_count].set_ylabel('Total Sales')

  # Plot stock_final vs time on the same subplot
  axes[ax_count].plot(product_data['periodo'], product_data['stock_final'], label='Stock Final')

  # Add legend
  axes[ax_count].legend()

  # Increment the counter
  ax_count += 1

  # Check if we've reached the last subplot
  if ax_count == len(axes):
    break

# Adjust the spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

"""**Los niveles de stock a menudo fluctúan inversamente a las ventas**: Cuando las ventas suben, el stock tiende a disminuir. Después de caídas en ventas, el stock suele acumularse

Algunos productos muestran comportamientos de stock interesantes:
El producto 20011 tiene un pico de stock inusual. Productos como 20004 y 20005 muestran niveles de stock crecientes hacia el final

La gestión de inventario varía: Algunos productos mantienen un stock relativamente estable (ej. 20001)
Otros muestran niveles de stock más volátiles (ej. 20019)

Diversas etapas del ciclo de vida del producto: Productos maduros con ventas estables (ej. 20013)
Productos en crecimiento (ej. 20009)
Productos en declive (ej. 20012)

### Análisis de customer_id
"""

# Podemos de primera mano hacer un ranking de los top 20 customer_id por las "tn" acumuladas
df_top_20_product_id_tn = sell_in.groupby('product_id')['tn'].sum().sort_values(ascending=False).head(20)
df_top_20_product_id_tn

"""*   Los TOP 20 clientes representan una parte significativa del total de toneladas vendidas (tn), lo que indica que **estos clientes son cruciales para el negocio.**

*   Algunos productos aparecen consistentemente en los TOP 10 de múltiples clientes, lo que sugiere que estos **productos son muy populares y demandados**.

# Modelo Gated Recurrent Unit (GRU)

Las Unidades Recurrentes con Puerta (GRU, por sus siglas en inglés) son una mejora sobre las RNN tradicionales, introducidas para manejar de manera más efectiva las dependencias a largo plazo en secuencias de datos.

A continuación compartimos los diferentes modelos experimentados, algunos (pocos) de manera exitosa en lo que refiere a la finalizacion sin errores de la simulacion y otros (varios) que hemos intentado y no llegaron a proveer una prediccion apta para Kaggle.

El modelo que decidimos consolidar en este entregable, enfrenta el problema de la predicción dividiendo el dataset en 6 grupos o clusters según la disponibilidad de datos indentificada durante el EDA.

## Modelo con Clusters

### 1) PRODUCTOS CON **6** **MESES** DE **DATOS** **CONTINUOS**
"""

monthly_data = ventas_df_productos_especificados

monthly_data

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definir el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_hasta_6_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_6 meses.txt'))
productos_hasta_6_meses_df.reset_index(drop=False, inplace=False)
productos_hasta_6_meses_df

# Para poder controlar luego que todos los productos aparezcan el la predicciones, contamos las filas en productos_hasta_6_meses_df

productos_hasta_6_meses_df.shape[0]

# Convertimos productos_hasta_6_meses_df en una lista

productos_hasta_6_meses_list = productos_hasta_6_meses_df['product_id'].tolist()

#Filtramos en monthly_data los 46 productos con 6 meses de datos
productos_especificos = productos_hasta_6_meses_list
monthly_data_6meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_data_6meses

#Instalamos el paquete Keras Tuner
!pip install keras_tuner

#Instalamos 3 paquetes que utilizaremos en nuestro modelo GRU
!pip install dtaidistance tslearn hyperopt

"""Procedemos ahora con el modelo de GRU para estos prodcutos con 6 meses de datos."""

# Importamos las librerías necesarias
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Listamos las semillas para la reproducibilidad
seeds = [232323, 123456, 7891011]

# Definimos gap_months y sequence_length como variables globales
gap_months = 1
sequence_length = 3

# Cargamos el dataset
data = monthly_data_6meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y crear la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la Función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [10, 20]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [2, 4])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=20, trials=trials)
    best['units'] = [10, 20][best['units']]
    best['sequence_length'] = [2, 4][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))  # Cambiado a 1 característica
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calculamos el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construimos el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=15, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos Función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

 # Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)
    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}
    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]
    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=10, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)
    return {'loss': tfe, 'status': STATUS_OK}

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Mostramos las predicciones promedio
print(average_predictions)

# Guardamos las predicciones promedio a un archivo CSV
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_6meses_promedio.csv', index=False)

# Filtramos y renombramos las columnas
predicciones_febrero_6meses_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones_febrero_6meses_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

#Visulizamos el df resultante
predicciones_febrero_6meses_df

# Nos aseguramos que la cantidad de filas en predicciones_febrero_6meses_df son las MISMAS que las filas en productos_hasta_6_meses_df para asegurar que no falta ningun producto.

# Compare the number of rows in both DataFrames
if len(predicciones_febrero_6meses_df) == len(productos_hasta_6_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones_febrero_6meses_df, productos_hasta_6_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones_febrero_6meses_df):
    print("All product_id values in predicciones_febrero_6meses_df are present in productos_hasta_6_meses_df.")
else:
    print("Some product_id values in predicciones_febrero_6meses_df are missing from productos_hasta_6_meses_df.")

#Exportamos las predicciones para este grupo de productos en un .csv que luego usaremos para combinar todos los productos en el archivo de subida a Kaggle.
predicciones_febrero_6meses_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_6meses_promedio.csv', index=False)

"""### 2) PRODUCTOS CON **24** **MESES** DE **DATOS** **DISCONTINUOS**

**2-** A continuación compartimos el modelo GRU que implementamos en los productos con **24 meses de datos continuos**. La idea inicial era poder hacer un finetunning en cada grupo de productos a fin de poder incorporar feature ingineering especifica en cada caso; sin embargo por limitaciones en la RAM disponible (y otras desafortunadas experiencias en maquinas virtuales que se desconectaron) tuvimo que limitar la cantidad de parametrso a experimentar (semillas, hiperparametros, features, etc) con el objetivo de poder llegar a que el modelo finalice la corrida.
"""

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definimos el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_hasta_24_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_24 meses.txt'))
productos_hasta_24_meses_df.reset_index(drop=False, inplace=False)
productos_hasta_24_meses_df

# Convertimos productos_hasta_24_meses_df en una lista

productos_hasta_24_meses_list = productos_hasta_24_meses_df['product_id'].tolist()

##Filtramos en monthly_data los productos con 24 meses de datos
productos_especificos = productos_hasta_24_meses_list
monthly_data_24meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_data_24meses

# Importamos las librerías necesarias para este modelo
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Listamos las semillas para la reproducibilidad - en este caso usamos al final 2 semillas porque fue con lo que pudimos finalizar la corrida
seeds = [232323, 7891011]

# Definimos gap_months y sequence_length como variables globales
gap_months = 1
sequence_length = 2

# Cargamos el dataset
data = monthly_data_24meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y creamos la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la Función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construir el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=6, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos Función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

    # Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)

    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}

    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]

    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=5, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)

    return {'loss': tfe, 'status': STATUS_OK}

#Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [3, 9]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [1, 3])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=8, trials=trials)
    best['units'] = [3, 9][best['units']]
    best['sequence_length'] = [1, 3][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))  # Cambiado a 1 característica
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calculamos el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Mostramos las predicciones promedio
print(average_predictions)

# Guardamos las predicciones promedio a un archivo CSV para luego combinar todo en un archivo que contento todos los productos
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_24meses_promedio.csv', index=False)

# Filtramos y renombramos las columnas del data frame de salida
predicciones24_febrero_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones24_febrero_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

# Nos aseguramos que el numero de productos en predicciones24_febrero_df son iguales a los de productos_hasta_24_meses_df

# Compare the number of rows in both DataFrames
if len(predicciones24_febrero_df) == len(productos_hasta_24_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones24_febrero_df, productos_hasta_24_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones24_febrero_df):
    print("All product_id values in predicciones24_febrero_df are present in productos_hasta_24_meses_df.")
else:
    print("Some product_id values in predicciones24_febrero_df are missing from productos_hasta_24_meses_df.")

# Exportamos predicciones24_febrero_df a un archivo .csv in /content/drive/MyDrive/Austral - Data Mining/Lab III/

predicciones24_febrero_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones24_promedio.csv', index=False)

"""### 3) PRODUCTOS CON **12** **MESES** DE **DATOS** **CONTINUOS**

**3-** A continuación compartimos el modelo GRU que implementamos en los productos con **12 meses de datos continuos**. La idea inicial era poder hacer un finetunning en cada grupo de productos a fin de poder incorporar feature ingineering especifica en cada caso; sin embargo por limitaciones en la RAM disponible (y otras desafortunadas experiencias en maquinas virtuales que se desconectaron) tuvimo que limitar la cantidad de parametrso a experimentar (semillas, hiperparametros, features, etc) con el objetivo de poder llegar a que el modelo finalice la corrida.
"""

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definimos el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_hasta_12_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_12 meses.txt'))
productos_hasta_12_meses_df.reset_index(drop=False, inplace=False)
productos_hasta_12_meses_df

# Pasamos productos_hasta_12_meses_df a una lista

productos_hasta_12_meses_list = productos_hasta_12_meses_df['product_id'].tolist()

#Filtramos de los productos especificados, aquellos con 12 meses de datos
productos_especificos = productos_hasta_12_meses_list
monthly_data_12meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_data_12meses

#contamos los producy_id en monthly_data_12meses

monthly_data_12meses['product_id'].value_counts()

# Importamos las librerías necesarias
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Lista de semillas para la reproducibilidad
seeds = [232323, 7891011]

# Definimos gap_months y sequence_length como variables globales
gap_months = 1
sequence_length = 2

# Cargamos el dataset
data = monthly_data_12meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y crear la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la Función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construimos el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=6, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos la Función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

    #Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)

    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}

    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]

    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=5, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)

    return {'loss': tfe, 'status': STATUS_OK}

# Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [3, 9]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [1, 3])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=8, trials=trials)
    best['units'] = [3, 9][best['units']]
    best['sequence_length'] = [1, 3][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))  # Cambiado a 1 característica
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calcular el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Mostramos las predicciones promedio
print(average_predictions)

# Guardamos las predicciones promedio a un archivo CSV
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_12meses_promedio.csv', index=False)

# Filtramos y renombramos columnas
predicciones12_febrero_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones12_febrero_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

#Visualizamos como quedo el df de salida
predicciones12_febrero_df

# Nos aseguramos que la cantidad de productos en predicciones12_febrero_df son iguales que en productos_hasta_12_meses_df

# Compare the number of rows in both DataFrames
if len(predicciones12_febrero_df) == len(productos_hasta_12_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones12_febrero_df, productos_hasta_12_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones12_febrero_df):
    print("All product_id values in predicciones12_febrero_df are present in productos_hasta_12_meses_df.")
else:
    print("Some product_id values in predicciones12_febrero_df are missing from productos_hasta_12_meses_df.")

# Verificamos si faltan productos nuevamente

missing_rows = productos_hasta_12_meses_df[~productos_hasta_12_meses_df['product_id'].isin(predicciones12_febrero_df['product_id'])]
print(missing_rows)

# Exportamos predicciones24_febrero_df a un .csv en /content/drive/MyDrive/Austral - Data Mining/Lab III/

predicciones12_febrero_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones12_promedio.csv', index=False)

"""### 4) PRODUCTOS CON **18** **MESES** DE **DATOS** **CONTINUOS**

**4-** A continuación compartimos el modelo GRU que implementamos en los productos con hasta **18 meses de datos continuos**. La idea inicial era poder hacer un finetunning en cada grupo de productos a fin de poder incorporar feature ingineering especifica en cada caso; sin embargo por limitaciones en la RAM disponible (y otras desafortunadas experiencias en maquinas virtuales que se desconectaron) tuvimo que limitar la cantidad de parametrso a experimentar (semillas, hiperparametros, features, etc) con el objetivo de poder llegar a que el modelo finalice la corrida.
"""

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definimos el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_hasta_18_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_18 meses.txt'))
productos_hasta_18_meses_df.reset_index(drop=False, inplace=False)
productos_hasta_18_meses_df

#Pasamos productos_hasta_18_meses_df a una lista
productos_hasta_18_meses_list = productos_hasta_18_meses_df['product_id'].tolist()

#Filtramos los productos especificados que tiene 18 meses de datos
productos_especificos = productos_hasta_18_meses_list
monthly_data_18meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_data_18meses

#Contamos los datos/meses para cada productos a fin de tenerlo en cuneta a los hora de realizar FE
monthly_data_18meses['product_id'].value_counts()

# Importamos las librerías necesarias
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Definimos la lista de semillas para la reproducibilidad
seeds = [232323, 7891011]

# Definimos gap_months y sequence_length como variables globales
gap_months = 1
sequence_length = 2

# Cargamos el dataset
data = monthly_data_18meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y creamos la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construimos el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=6, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos la función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

    # Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)

    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}

    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]

    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=5, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)

    return {'loss': tfe, 'status': STATUS_OK}

# Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [3, 9]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [1, 3])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=8, trials=trials)
    best['units'] = [3, 9][best['units']]
    best['sequence_length'] = [1, 3][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calculamos el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Guardamos las predicciones promedio a un archivo CSV
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18meses_promedio.csv', index=False)

# Filtramos y renombramos columnas
predicciones18_febrero_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones18_febrero_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

#Visulizamos como quedo el df
predicciones18_febrero_df

# Nos aseguramos que la cantidad de productos en predicciones18_febrero_df sin IGUALES a las de productos_hasta_18_meses_df

# Compare the number of rows in both DataFrames
if len(predicciones18_febrero_df) == len(productos_hasta_18_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones18_febrero_df, productos_hasta_18_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones18_febrero_df):
    print("All product_id values in predicciones18_febrero_df are present in productos_hasta_18_meses_df.")
else:
    print("Some product_id values in predicciones18_febrero_df are missing from productos_hasta_18_meses_df.")

# Exportamos predicciones18_febrero_df en un .csv in /content/drive/MyDrive/Austral - Data Mining/Lab III/

predicciones18_febrero_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones18_promedio.csv', index=False)

"""### 5) PRODUCTOS CON **18** y **21** **MESES** DE **DATOS** **CONTINUOS**

**5-** A continuación compartimos el modelo GRU que implementamos en los productos que tienen entre **18 y 21 meses de datos continuos**. La idea inicial era poder hacer un finetunning en cada grupo de productos a fin de poder incorporar feature ingineering especifica en cada caso; sin embargo por limitaciones en la RAM disponible (y otras desafortunadas experiencias en maquinas virtuales que se desconectaron) tuvimo que limitar la cantidad de parametrso a experimentar (semillas, hiperparametros, features, etc) con el objetivo de poder llegar a que el modelo finalice la corrida.
"""

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definimos el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_18_21_Cont_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_18_21_Cont_meses.txt'))
productos_18_21_Cont_meses_df.reset_index(drop=False, inplace=False)
productos_18_21_Cont_meses_df

# Pasamos productos_18_21_Cont_meses_df a una lista

productos_18_21_Cont_meses_list = productos_18_21_Cont_meses_df['product_id'].tolist()

# Filtramos de los productos especificados aquellos con datos entre 18 y 21 meses
productos_especificos = productos_18_21_Cont_meses_list
monthly_18_21_Cont_meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_18_21_Cont_meses

##Contamos los datos/meses para cada productos a fin de tenerlo en cuenta a los hora de realizar FE
monthly_18_21_Cont_meses['product_id'].value_counts()

# Importamos las librerías necesarias
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Listamos las semillas para la reproducibilidad
seeds = [232323, 7891011]

# Definimos gap_months y sequence_length como variables globales
gap_months = 1
sequence_length = 2

# Cargamos el dataset
data = monthly_18_21_Cont_meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y crear la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la Función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construimos el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=6, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos la Función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

    #Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)  # Reshape to 2D

    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}

    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]

    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=5, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)

    return {'loss': tfe, 'status': STATUS_OK}

# Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [3, 9]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [1, 3])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=8, trials=trials)
    best['units'] = [3, 9][best['units']]
    best['sequence_length'] = [1, 3][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))  # Cambiado a 1 característica
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calcular el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Mostramos las predicciones promedio
print(average_predictions)

# Guardamos las predicciones promedio a un archivo CSV
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_21_meses_promedio.csv', index=False)

# Filtramos y renombramos columnas
predicciones18_21_Cont_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones18_21_Cont_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

#Visualizazmos como quedo el df
predicciones18_21_Cont_df

# Nos aseguramos que las filas en predicciones18_21_Cont_df son iguales a las filas en productos_18_21_Cont_meses_df

# Compare the number of rows in both DataFrames
if len(predicciones18_21_Cont_df) == len(productos_18_21_Cont_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones18_21_Cont_df, productos_18_21_Cont_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones18_21_Cont_df):
    print("All product_id values in predicciones18_21_Cont_df are present in productos_18_21_Cont_meses_df.")
else:
    print("Some product_id values in predicciones18_21_Cont_df are missing from productos_18_21_Cont_meses_df.")

# Exportamos predicciones24_febrero_df a un .csv in /content/drive/MyDrive/Austral - Data Mining/Lab III/

predicciones18_21_Cont_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_21_Cont_promedio.csv', index=False)

"""### 6) PRODUCTOS CON **18** y **24** **MESES** DE **DATOS** **DISCONTINUOS**

**6-** A continuación compartimos el modelo GRU que implementamos en el último grupo de prodcutos, aquellos con **18 y 24 meses de datos pero discontinuos, es decir, poseen meses sin datos**. La idea de separar este grupo de productos del resto lo fundamentamos desde la perspectiva del potencial y desconocido impacto que podrain tener datos faltantes sobre GRU.
De igual manera, para este grupo de datos, la idea inicial era poder hacer un finetunning un poco mas especifico, a fin de poder incorporar feature ingineering especifica en cada caso; sin embargo por limitaciones en la RAM disponible (y otras desafortunadas experiencias en maquinas virtuales que se desconectaron) tuvimo que limitar la cantidad de parametrso a experimentar (semillas, hiperparametros, features, etc) con el objetivo de poder llegar a que el modelo finalice la corrida.
"""

from pickle import FALSE
import pandas as pd
import os
import numpy as np

# Definimos el directorio de los scripts y datos
directorio_script = '/content/drive/MyDrive/Austral - Data Mining/Lab III/'
productos_18_24_Disc_meses_df = pd.read_csv(os.path.join(directorio_script, 'productos_18_24_Disc_meses.txt'))
productos_18_24_Disc_meses_df.reset_index(drop=False, inplace=False)
productos_18_24_Disc_meses_df

#Pasamos productos_18_21_Cont_meses_df a una lista
productos_18_24_Disc_meses_list = productos_18_24_Disc_meses_df['product_id'].tolist()

#Filtramos los productos especificados que poseen esta caracteristica en los datos
productos_especificos = productos_18_24_Disc_meses_list
monthly_18_24_Disc_meses = monthly_data[monthly_data['product_id'].isin(productos_especificos)]
monthly_18_24_Disc_meses

#Contamos la cantidad de productos
monthly_18_24_Disc_meses['product_id'].value_counts()

# Importamos las librerías necesarias
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from dtaidistance import dtw
from tslearn.clustering import TimeSeriesKMeans
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.preprocessing import StandardScaler

# Definimos la función para establecer la semilla
def set_seed(seed_value):
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)

# Listamos de semillas para la reproducibilidad
seeds = [232323, 7891011]

# Definimos gap_months y sequence_length como variable global
gap_months = 1
sequence_length = 2

# Cargamos el dataset
data = monthly_18_24_Disc_meses
data['periodo'] = pd.to_datetime(data['periodo'])
data.sort_values(by=['product_id', 'periodo'], inplace=True)

# Identificamos la posición de 'tn' y crear la lista de características adicionales
tn_index = data.columns.get_loc('tn')
features = [col for i, col in enumerate(data.columns) if i != tn_index and col != 'periodo' and col != 'product_id']

# Normalizamos las características para cada product_id
scalers = {}
for pid in data['product_id'].unique():
    scaler = StandardScaler()
    pid_data = data.loc[data['product_id'] == pid, features + ['tn']]  # Incluir 'tn' en la normalización
    if not pid_data.empty:
        data.loc[data['product_id'] == pid, features + ['tn']] = scaler.fit_transform(pid_data)
        scalers[pid] = scaler

# Determinamos la longitud máxima de las series temporales
max_length = max(len(data.loc[data['product_id'] == pid, 'tn']) for pid in data['product_id'].unique())

# Creamos una matriz con las series temporales normalizadas y realizar padding
series = []
for pid in data['product_id'].unique():
    series_temp = data.loc[data['product_id'] == pid, 'tn'].values
    series_padded = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
    series.append(series_padded)
series = np.array(series)

# Definimos la Función para crear secuencias
def create_sequences(data, sequence_length, gap_months):
    X, y = [], []
    # Ajustar el rango para evitar secuencias vacías
    for i in range(len(data) - sequence_length - gap_months + 1):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length + gap_months - 1])
    return np.array(X), np.array(y)

# Definimos la Función para calcular el Total Forecast Error (TFE)
def total_forecast_error(y_true, y_pred):
    error_absoluto = np.sum(np.abs(y_true - y_pred))
    suma_valores_reales = np.sum(np.abs(y_true))
    # Manejar el caso donde la suma de valores reales es cero
    if suma_valores_reales == 0:
        return 0
    tfe = error_absoluto / suma_valores_reales
    return tfe

# Construimos el modelo GRU con múltiples características de entrada
def build_gru_model(input_shape, units=6, dropout_rate=0.2, learning_rate=0.05):
    model = Sequential([
        GRU(units, activation='relu', recurrent_activation='sigmoid', return_sequences=True, input_shape=input_shape),
        Dropout(dropout_rate),
        GRU(units // 2, activation='relu', recurrent_activation='sigmoid'),
        Dropout(dropout_rate),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Definimos la Función para realizar la optimización de hiperparámetros
def hyperopt_objective(params, data, gap_months):
    sequence_length = params['sequence_length']
    units = params['units']
    dropout_rate = params['dropout_rate']
    learning_rate = params['learning_rate']

    #Convertimos los datos a una forma 2D antes de pasarlos a una función create_sequences
    X, y = create_sequences(data['tn'].values.reshape(-1, 1), sequence_length, gap_months)  # Reshape to 2D

    if len(X) == 0 or len(y) == 0:
        return {'loss': float('inf'), 'status': STATUS_FAIL}

    X_train, X_val = X[:-1], X[-1:]
    y_train, y_val = y[:-1], y[-1:]

    model = build_gru_model((X_train.shape[1], X_train.shape[2]), units, dropout_rate, learning_rate)
    model.fit(X_train, y_train, epochs=5, verbose=0)
    y_val_pred = model.predict(X_val)
    tfe = total_forecast_error(y_val, y_val_pred)

    return {'loss': tfe, 'status': STATUS_OK}

# Definimos la Función para optimizar hiperparámetros
def optimize_hyperparameters(data, gap_months):
    space = {
        'units': hp.choice('units', [3, 9]),
        'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.3),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),
        'sequence_length': hp.choice('sequence_length', [1, 3])
    }

    trials = Trials()
    best = fmin(lambda params: hyperopt_objective(params, data, gap_months), space, algo=tpe.suggest, max_evals=8, trials=trials)
    best['units'] = [3, 9][best['units']]
    best['sequence_length'] = [1, 3][best['sequence_length']]

    return best

# Optimizamos los hiperparámetros
best_params = optimize_hyperparameters(data, gap_months)

# Definimos la Función para predecir directamente el segundo mes
def predecir_directamente_segundo_mes(data, mes_2, best_params, gap_months):
    predicciones = []

    sequence_length = best_params['sequence_length']

    for pid in data['product_id'].unique():
        product_data = data[data['product_id'] == pid]

        if len(product_data) < sequence_length + gap_months:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        series_temp = product_data['tn'].values
        series_temp = np.pad(series_temp, (0, max_length - len(series_temp)), 'constant', constant_values=(0,))
        X, y = create_sequences(series_temp.reshape(-1, 1), sequence_length, gap_months)
        if len(X) == 0 or len(y) == 0:
            mean_tn = product_data['tn'].mean()
            predicciones.append({'product_id': pid, f'tn_{mes_2}': mean_tn})
            continue

        X = X.reshape((X.shape[0], X.shape[1], 1))  # Cambiado a 1 característica
        model = build_gru_model((X.shape[1], X.shape[2]), best_params['units'], best_params['dropout_rate'], best_params['learning_rate'])
        model.fit(X, y, epochs=20, verbose=0)

        ultimo_periodo = series_temp[-sequence_length:].reshape((1, sequence_length, 1))
        prediccion_mes2_scaled = model.predict(ultimo_periodo)
        prediccion_mes2_scaled_value = prediccion_mes2_scaled[0, 0]

        zero_filled = np.zeros((1, 1))
        zero_filled[0, -1] = prediccion_mes2_scaled_value
        prediccion_mes2 = scalers[pid].inverse_transform(zero_filled)[0, -1]

        predicciones.append({'product_id': pid, f'tn_{mes_2}': prediccion_mes2})

    predicciones_df = pd.DataFrame(predicciones)
    return predicciones_df

# Iteramos sobre cada semilla y calcular el promedio de las predicciones
all_predictions = []
for seed in seeds:
    set_seed(seed)
    best_params = optimize_hyperparameters(data, gap_months)
    predicciones = predecir_directamente_segundo_mes(data, '2020-02', best_params, gap_months)
    all_predictions.append(predicciones)

# Promediamos las predicciones
average_predictions = pd.concat(all_predictions).groupby('product_id').mean().reset_index()

# Mostramos las predicciones promedio
print(average_predictions)

# Guardamos las predicciones promedio a un archivo CSV
average_predictions.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_21_Disc_meses_promedio.csv', index=False)

# Filtramos y renombramos columnas
predicciones18_24_Disc_df = average_predictions[['product_id', 'tn_2020-02']]
predicciones18_24_Disc_df.rename(columns={'tn_2020-02': 'tn'}, inplace=True)

#Visualizamos como quedo el df
predicciones18_24_Disc_df

# Nos aseguramos que las filas en predicciones18_24_Disc_df son IGUALES en productos_18_24_Disc_meses_df

# Compare the number of rows in both DataFrames
if len(predicciones18_24_Disc_df) == len(productos_18_24_Disc_meses_df):
    print("Both DataFrames have the same number of rows.")
else:
    print("The number of rows in both DataFrames is different.")

# Compare the product_id values in both DataFrames
merged_df = pd.merge(predicciones18_24_Disc_df, productos_18_24_Disc_meses_df, on='product_id', how='inner')

if len(merged_df) == len(predicciones18_24_Disc_df):
    print("All product_id values in predicciones18_24_Disc_df are present in productos_18_24_Disc_meses_df.")
else:
    print("Some product_id values in predicciones18_24_Disc_df are missing from productos_18_24_Disc_meses_df.")

# Exportamos predicciones24_febrero_df en un .csv in /content/drive/MyDrive/Austral - Data Mining/Lab III/

predicciones18_24_Disc_df.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_24_Disc_promedio.csv', index=False)

"""En este punto hemos finalizado la prediccion de los productos segun la cantidad de datos disponibles para cada grupo.

Destacamos que el fine tunning de cada hiperparametro tuvo que ser limitado lamentablemnte al hardware disponible y no pudimos abrir el abanico de pruebas y sensibilidades que quizas hubieran resultado en mejores predicciones por grupo (o en mayor overfitting...)

"""

# Consolidamos todas las salidas en un solo archivo para subir a Kaggle

import pandas as pd

# Read the CSV files
predicciones12_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones12_promedio.csv')
predicciones24_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones24_promedio.csv')
predicciones_6meses_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_6meses_promedio.csv')
predicciones18_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_predicciones18_promedio.csv')
predicciones18_21_Cont_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_21_Cont_promedio.csv')
predicciones18_24_Disc_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/GRU2+11_18_24_Disc_promedio.csv')

# Combinamos el DataFrame
predicciones_febrero = pd.concat([predicciones12_febrero, predicciones24_febrero, predicciones_6meses_febrero, predicciones18_febrero, predicciones18_21_Cont_febrero, predicciones18_24_Disc_febrero])

# Acomodamos el DataFrame por product_id
predicciones_febrero.sort_values(by='product_id', inplace=True)

# Guardamos el DataFrame en un nuevo archivo .CSV para subir a Kaggle
predicciones_febrero.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/Kaggle_GRU2+11_semillerio_7_feb.csv', index=False)



"""Manipulación matemática de los resultados para mejorar el archivo a subir a Kaggle.

A continuación agregamos algunos intentos matemáticos para generar algunas submissions para Kaggle, usando como referencia las "tn" totales resultante de la media de los ultimos 12 meses para cada producto.
La idea es mantener la proporcionalidad de cada producto y mantenernos cerca del total de "tn".
En el leaderbord publico se obervaró una mejora en el puntaje despues de este "post-procesamiento".
"""

#Visualizamos el df resultante de la prediccion total
predicciones_febrero

# Hacemos la suma total de "tn" en predicciones_febrero

tn_sum = predicciones_febrero['tn'].sum()
print(tn_sum)

# Ahora, manteniendo constante la proporcion de cada producto en el total, realizamos cambios para que el total de la suma sea 30.644 tn

def adjust_proportions(data, target_sum):
  """
  Adjusts the values in a column of a DataFrame to have a specific sum while maintaining the original proportions.

  Args:
    data: A Pandas DataFrame.
    target_sum: The desired sum of the specified column.

  Returns:
    A Pandas DataFrame with adjusted values.
  """
  current_sum = data['tn'].sum()
  adjustment_factor = target_sum / current_sum
  data['tn'] = data['tn'] * adjustment_factor
  return data

predicciones_febrero_adj = adjust_proportions(predicciones_febrero, 30644)
print(predicciones_febrero_adj['tn'].sum())

#Visualizamos la salida
predicciones_febrero_adj

# Exportamos la salida para subir a Kaggle
predicciones_febrero_adj.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/Kaggle_GRU2+11_semillerio_7_adj.csv', index=False)

"""Continuamos luego con mas manipulación matemática de la salida original, esta vez agregando nuestra interpretacion subjetiva de la realidad economica que se vivia en la epoca cuando tuvo lugar este ejercicio y que no pudimos reproducir con los modelos que intentamos, en epoca de crisis las ventas deberian de tener una tendencia a la baja...
Por lo que experimentamos en varias oportunidades, subiendo los resultados a Kaggle, fue forzar la caida de ventas en productos con mas de 500 tn de manera matemática y compensar con un aumento relativo en las ventas de aquellos con menos de 500 tn a fin de mantener el total cerca del 30.600 tn que nos arroja el caso de la sumatoria de las medias de cada prodcuto. Los resultados en Kaggle de esta manipulacíon mejoró los resultados en el Public Leaderboard.   

"""

# Manteniendo las proporciones iguales para cada producto, cambiamos las tn para mantener el total de la suma en 30.644
#y Para productos con mas de 500 tn penalizamos un 5% mientras aumentamos las tn en productos con menos de 500 tn, manteniendo el total

import pandas as pd

def adjust_proportions(data, target_sum):
  """
  Adjusts the values in a column of a DataFrame to have a specific sum while maintaining the original proportions.

  Args:
    data: A Pandas DataFrame.
    target_sum: The desired sum of the specified column.

  Returns:
    A Pandas DataFrame with adjusted values.
  """
  current_sum = data['tn'].sum()
  adjustment_factor = target_sum / current_sum
  data['tn'] = data['tn'] * adjustment_factor
  return data

def adjust_tn_values(data):
  """
  Adjusts tn values based on product tn sum.

  Args:
    data: A Pandas DataFrame.

  Returns:
    A Pandas DataFrame with adjusted tn values.
  """
  # Separate products with tn sum greater and less than 500
  high_tn_products = data[data['tn'] > 500]
  low_tn_products = data[data['tn'] <= 500]

  # Reduce tn by 10% for high_tn_products
  high_tn_products['tn'] = high_tn_products['tn'] * 0.95

  # Increase tn for low_tn_products to maintain total sum
  low_tn_products['tn'] = low_tn_products['tn'] * (target_sum - high_tn_products['tn'].sum()) / low_tn_products['tn'].sum()

  # Combine adjusted DataFrames
  data = pd.concat([high_tn_products, low_tn_products])

  return data

# Read the CSV file
predicciones_febrero = pd.read_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/Kaggle_GRU2+11_semillerio_3_final.csv')

# Set target sum
target_sum = 30644

# Adjust proportions
predicciones_febrero_adj = adjust_proportions(predicciones_febrero, target_sum)

# Adjust tn values based on product tn sum
predicciones_febrero_final = adjust_tn_values(predicciones_febrero_adj)

# Save the final DataFrame to a new CSV file
predicciones_febrero_final.to_csv('/content/drive/MyDrive/Austral - Data Mining/Lab III/Kaggle_GRU2+11_semillerio_7_final.csv', index=False)

predicciones_febrero_final

"""# Otros modelos GRU experimentados

Compartimos a continuación varios modelos adicionales que fueron experimentados a lo largo de estas últimas semanas. Otros varios modelos decidimos dejar afuera para evitar efectos secundarios en el lector y aprobador de este trabajo.

a) **Predicción directa vs predicción secuencial**

Predecir directamente febrero vs predecir enero y despues agregar los resultados al dataset para predecir febrero.

Nuestra intuición era que al utilizar información adicional (las predicciones de enero) para hacer la predicción de febrero, potencialmente esto podía mejorar la precisión del modelo. La desventaja es la acumulación de errores: si las predicciones de enero son inexactas, esos errores afectan la predicción de febrero. Los resultados de las pruebas realizadas indicaron que la performance de los modelos son parecidos

b) **Diferentes valores de sequence_length (longitud de la secuencia)**

En GRU, la longitud de la secuencia determina cuánto tiempo en el pasado se considera para hacer una predicción. Es decir, una secuencia corta puede captar tendencias a corto plazo y patrones estacionales recientes, mientras que una secuencia larga puede captar tendencias a largo plazo y patrones estacionales más amplios.

Estamos trabajando con datos de ventas que muestran una fuerte estacionalidad anual en algunos productos. Por ejemplo, cuando trabajamos con el TOP 12 de productos en clase, para el producto 20001 observamos que todos los febreros son menores a los eneros, o al menos se observa un descenso en los tres meses anteriores a febrero. En ese sentido, probamos diferentes longitudes de secuencia para capturar estos patrones estacionales.

Probamos con una longitud de secuencia de 12 meses para capturar la estacionalidad anual, pero también probamos con longitudes de secuencia de 3 meses y 6 meses.

Por los resultados en el público en Kaggle, nuestra conclusión es que una longitud de secuencia de 3 meses parece ser la mejor para capturar las tendencias recientes sin añadir demasiado ruido o complejidad. Para captar los factores a largo plazo, utilizamos variables de retardo (lags), lo que permite al modelo considerar datos históricos sin necesidad de aumentar excesivamente la longitud de la secuencia.

c) **Ajustar la cantidad de capas del modelo GRU**

Un problema que encontramos fue la adaptación del modelo para predecir mensualmente cuando agrupábamos los datos por periodo, product_id y customer_id. Nuestros modelos terminaban prediciendo el tn por customer_id, y la cantidad de tn era muy baja, productos que deberían tener 1400 tn lograban apenas 50tn.

Para solucionar esto, agrupamos los datos solo por periodo y product_id, e hicimos un one-hot encoding de customer_id. Como resultado, llegamos a tener alrededor de 600 características en total (sumando las creadas por nosotros).

Con tantas características, probablemente necesitábamos un modelo más profundo. Así que probamos utilizando múltiples capas GRU con un número decreciente de unidades. Por ejemplo:

model = Sequential([
GRU(256, activation='relu', return_sequences=True, input_shape=(sequence_length, 628),
kernel_regularizer=l2(l2_lambda), recurrent_regularizer=l2(l2_lambda)),
Dropout(0.3),
GRU(128, activation='relu', return_sequences=True,
kernel_regularizer=l2(l2_lambda), recurrent_regularizer=l2(l2_lambda)),
Dropout(0.3),
GRU(64, activation='relu',
kernel_regularizer=l2(l2_lambda), recurrent_regularizer=l2(l2_lambda)),
Dropout(0.3),
Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),
Dense(1)
])

model.compile(optimizer='adam', loss='mse')

d) **Entrenar un modelo separado para cada product_id vs entrenar un modelo general y despues realizar fine tunning**

Inicialmente diseñamos el código para entrenar un modelo separado para cada product_id. Esto inicialmente lo hcimos sin darnos cuenta, por no haber interpretado correctamente el código. Esto se puede ver en el bucle que itera sobre los product_id únicos:

for pid in data['product_id'].unique():
if pid in sequence_data and len(sequence_data[pid][0]) > 0:
X, y = sequence_data[pid]

model = create_model(input_shape=(sequence_length, len(features) + 1))
model.fit(X, y, epochs=20, validation_split=0.2, callbacks=[early_stopping], verbose=0)

Ahora bien, la pregunta que nos surgió fue: si quisiéramos relacionar las categorías de diferentes productos, ¿este modelo lo hace? Y la respuesta fue no. Cada modelo se entrenaba por separado y hacía predicciones de forma independiente para cada product_id. No existía ningún mecanismo en el código que permita que los modelos compartan información o aprendan relaciones entre diferentes productos.

Por lo tanto, lo que hicimos fue modificar el código para crear un modelo único que haga la relación entre todos los productos y luego mejorarlo con entrenamiento individual para cada producto. Comenzando con los pesos del modelo base, se realiza un "fine-tuning" para cada producto específico. Esta técnica se conoce comúnmente como "transfer learning" o aprendizaje por transferencia.

e) **Ponderación por importancia de product_id**

Implementar un sistema de ponderación donde cada producto tiene un peso específico basado en su importancia (como las toneladas vendidas). La idea detrás de esto es ponderar más los errores en los productos que tienen un mayor volumen de ventas, asumiendo que un error en un producto con mayores ventas es más significativo o costoso para la empresa. Primero, definimos cómo se calculaban los pesos, creamos un diccionario de pesos, y lo incorporamos en una función de pérdida personalizada. Luego, cuando compilamos el modelo, nos aseguramos de especificar la función de pérdida personalizada: model.compile(optimizer='adam', loss=weighted_loss). A nuestro entender mejoró un poco el modelo, sobre todo cuando lo ajustamos para que no alejara tanto del promedio de 12 meses.

f) **Dropout vs Early Stopping**

Implementamos Dropout, una técnica de regularización utilizada para prevenir el sobreajuste (overfitting) en redes neuronales. Básicamente, durante el entrenamiento, algunas neuronas de la red se "apagan" aleatoriamente en cada paso de actualización, lo que impide que la red se vuelva demasiado dependiente de rutas específicas de activación.

Además de esta técnica, también implementamos Early Stopping, que detiene el entrenamiento cuando el modelo deja de mejorar en el conjunto de validación, ahorrando así tiempo y recursos.

No se observó una mejora significativa en la performance.

g) **Modelos con Feature engineering**

1. Features Temporales

Mes y Año: Extraer el mes y el año del campo periodo para capturar la estacionalidad y las tendencias temporales.
Estacionalidad: Crear una variable que indique la estación del año (primavera, verano, otoño, invierno) según el mes.
Cuartos Comerciales: Crear variables para los cuartos comerciales.

2. Features Basados en Variables de Tendencias y Cambios

Promedio de Ventas en Toneladas
Volatilidad de Ventas: Desviación estándar de las ventas para cada producto.
Ventas Acumuladas en Toneladas
Ventas Anteriores (Lags)
Promedio móvil de ventas en toneladas por producto.
Cambio relativo de ventas en toneladas comparado con el periodo anterior
Tendencia a Largo Plazo: Tendencia lineal de las ventas a lo largo del tiempo para cada producto. Captura la tendencia general de aumento o disminución de las ventas.
Ventas Máximas y Mínimas por Producto: Identificar los meses con ventas máximas y mínimas para cada producto.

3. Features Basados en Cliente

Frecuencia de Compra: Calcular la frecuencia de compra del customer_id en los datos históricos.
Volumen de Compra Acumulado: Calcular el volumen total de compras realizadas por cada cliente.
Días desde la Última Compra: Número de días desde la última compra de un cliente.

4. Features Basados en Stock

Cambio de Stock: Calcular la diferencia entre stock_final y cust_request_qty para ver cómo cambian los niveles de inventario.
Ratio de Stock: Calcular el ratio de stock_final sobre cust_request_qty para entender la relación entre stock y ventas.
Promedio de Stock Final por Producto: El promedio de stock final para cada producto.


5. Features Basados en Producto

Categoría Completa: Concatenar cat1, cat2 y cat3 para crear una variable de categoría completa.
Precio por Unidad de Peso:  se puede calcular un precio por unidad de peso si se dispone de los precios.


6. Features Basados en Promociones

Impacto de Promociones: Crear una variable binaria que indique si el producto está en el plan de precios cuidados.
"""

# # # Promedio de ventas en toneladas por producto
# monthly_data['mean_tn'] = monthly_data.groupby('product_id')['tn'].transform('mean')

# # # Volatilidad de ventas (desviación estándar)
# monthly_data['std_tn'] = monthly_data.groupby('product_id')['tn'].transform('std')

# # Desviación Estándar Móvil: Utiliza la desviación estándar en una ventana móvil para medir la volatilidad de las ventas.
# monthly_data['rolling_std_3'] = monthly_data.groupby('product_id')['tn'].transform(lambda x: x.rolling(window=3).std())

# # Ventas acumuladas en toneladas por producto
# monthly_data['cumulative_tn'] = monthly_data.groupby('product_id')['tn'].cumsum()

# # # Crear ventas anteriores (lags)
# monthly_data['tn_lag_1'] = monthly_data.groupby('product_id')['tn'].shift(1)
# monthly_data['tn_lag_2'] = monthly_data.groupby('product_id')['tn'].shift(2)
# monthly_data['tn_lag_3'] = monthly_data.groupby('product_id')['tn'].shift(3)
# monthly_data['tn_lag_4'] = monthly_data.groupby('product_id')['tn'].shift(4)
# # monthly_data['tn_lag_5'] = monthly_data.groupby('product_id')['tn'].shift(5)
# # monthly_data['tn_lag_6'] = monthly_data.groupby('product_id')['tn'].shift(6)
# # monthly_data['tn_lag_7'] = monthly_data.groupby('product_id')['tn'].shift(7)
# # monthly_data['tn_lag_8'] = monthly_data.groupby('product_id')['tn'].shift(8)
# # monthly_data['tn_lag_9'] = monthly_data.groupby('product_id')['tn'].shift(9)
# # monthly_data['tn_lag_10'] = monthly_data.groupby('product_id')['tn'].shift(10)
# #monthly_data['tn_lag_11'] = monthly_data.groupby('product_id')['tn'].shift(11)
# monthly_data['tn_lag_12'] = monthly_data.groupby('product_id')['tn'].shift(12)

"""# "Ideas" pendientes de experimentación y Lecciones Aprendidas

Algunas cosas que nos quedaron pendientes de experimentación:

- Sensibilidad de los hiperparametros prediciendo Nov2019 o Dic2019
- Analisis por cliente
- Incrementar la Sensibilidad a Técnicas de Regularización (L1, L2, Dropout..) para reducir el Overfitting


Esta carencia de experimentación tambien nos ayudó a definir puntos muy importantes a tener en cuenta en próximos desafios con series temporales y que hemos aprendido a traves del dolor y sufrimiento de la experimentación en modo solista. Algunos de estos puntos son:

- Trazabilidad y prolijidad en los modelos experimentados; especialmente cuando trabajamos en grupo y cada experimentador posee libertad ilimitada en la experimentación. A las 2 semanas de haber comenzado cada integrante del equipo tenia no menos de 10 modelos sin poder en esa etapa poder ponderar cual era mejor que otro para continuar desarrollando.

- Focalizarnos en el problema y no en el código. Nuestra falta de computer literacy hizo que pasaramos mucho tiempo trantando de que los codigos nos dieran archivos aptos para submission.

# Conclusión

En este informe, se ha documentado el proceso de creación y evaluación de varios modelos de Redes Neuronales Recurrentes con Unidad de Memoria a Largo Plazo (GRU) para la predicción de una serie temporal.

A pesar de los esfuerzos y ajustes realizados, los resultados obtenidos con el modelo GRU no superaron los de un modelo de referencia basado en el promedio de los datos históricos.

**Resultados y Discusión**

**Evaluación del Modelo GRU**: Se emplearon métricas como el error cuadrático medio (MSE) y el error absoluto medio (MAE) para evaluar el desempeño del modelo GRU. Los resultados mostraron que el modelo GRU no capturó adecuadamente las dinámicas de la serie temporal, resultando en un desempeño por debajo de lo esperado (segun el Public Leaderboard).

**Comparación con el Modelo de Promedio**: El modelo de referencia, basado en el promedio de los valores históricos, mostró un rendimiento comparable o superior al del modelo GRU en términos de precisión de predicción (segun el Public Leaderboard). Esto sugiere que, para esta serie temporal específica, la complejidad del modelo GRU no aportó beneficios significativos.

**Factores Contribuyentes**

Varias razones pueden explicar por qué el modelo GRU no superó al promedio:

**Naturaleza de los Datos**: La serie temporal contiene patrones que no son fácilmente capturables por un modelo GRU, como tendencias estacionales fuertes o componentes aleatorios predominantes que no pudimos aislar o tratar de manera de reducir su influencia.

**Preprocesamiento y Características**: Es posible que los datos no hayan sido suficientemente procesados o que no se hayan considerado todas las características relevantes.

**Conclusiones Finales**

Aunque el modelo GRU es una herramienta poderosa para la predicción de series temporales, en este caso particular, no superó al método simple del promedio. Esto destaca la importancia de evaluar y comparar diferentes enfoques antes de implementar modelos más complejos.

En futuras investigaciones, consideramos que será útil explorar otros modelos y técnicas de preprocesamiento.

Esta experimentacion intensiva durante los ultimos 2 meses también resalta que los métodos simples y robustos pueden ofrecer soluciones competitivas con menor complejidad computacional y operativa.
"""